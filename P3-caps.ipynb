{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 200\n",
    "steps_per_epoch = 5000//batch_size\n",
    "summary = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_reader(part):\n",
    "    for i in range(10):\n",
    "        with open('Part3_{}_{}.csv'.format(i, part)) as f:\n",
    "            for line in f:\n",
    "                yield line.strip(), i\n",
    "\n",
    "def mnist_decoder(csv_line, label):\n",
    "    FIELD_DEFAULTS = [[0.0]]*(28*28)\n",
    "    with tf.variable_scope('DataSource'):\n",
    "        fields = tf.decode_csv(csv_line, FIELD_DEFAULTS)\n",
    "        im = tf.stack(fields)\n",
    "        im = tf.reshape(im, (28, 28, 1))\n",
    "        return im, tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('DataSource'):\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: mnist_reader('Train'),\n",
    "        # csv_line, label\n",
    "        (tf.string, tf.int32),\n",
    "        (tf.TensorShape([]), tf.TensorShape([])\n",
    "    )).map(mnist_decoder, num_parallel_calls=2) \\\n",
    "      .batch(batch_size) \\\n",
    "      .prefetch(1) \\\n",
    "      .repeat(epochs)\n",
    "    \n",
    "    dataset_val = tf.data.Dataset.from_generator(lambda: mnist_reader('Test'),\n",
    "        (tf.string, tf.int32),\n",
    "        (tf.TensorShape([]), tf.TensorShape([])\n",
    "    )).map(mnist_decoder, num_parallel_calls=2) \\\n",
    "      .batch(batch_size) \\\n",
    "      .prefetch(1)\n",
    "\n",
    "    iter_handle = tf.placeholder(tf.string, shape=[])\n",
    "    data_iterator = tf.data.Iterator.from_string_handle(iter_handle, dataset.output_types, dataset.output_shapes)\n",
    "    train_iterator = dataset.make_one_shot_iterator()\n",
    "    val_iterator = dataset_val.make_initializable_iterator()\n",
    "    val_init_op = data_iterator.make_initializer(dataset_val)\n",
    "    images, onehot_labels = data_iterator.get_next()\n",
    "\n",
    "with tf.variable_scope('CNN'):\n",
    "    convmaps = tf.keras.layers.Conv2D(16, (7,7), activation='tanh')(images)\n",
    "    features = tf.reshape(convmaps, (batch_size, 16*22*22))\n",
    "    fc1 = tf.keras.layers.Dense(128, activation='tanh')(features)\n",
    "    pred = tf.keras.layers.Dense(10, activation='sigmoid')(features)\n",
    "    \n",
    "with tf.variable_scope('LossFn'):\n",
    "    total_loss = tf.reduce_sum(tf.square(onehot_labels-pred), axis=1)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    global_step = tf.Variable(0)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "    train_op = optimizer.minimize(total_loss, global_step)\n",
    "    \n",
    "with tf.variable_scope('Metrics'):\n",
    "    labels = tf.to_int32(tf.argmax(onehot_labels, axis=1))\n",
    "    argmax_idx = tf.to_int32(tf.argmax(pred, axis=1))\n",
    "    epoch_loss_avg, epoch_loss_avg_update = tf.metrics.mean(total_loss)\n",
    "    epoch_accuracy, epoch_accuracy_update = tf.metrics.accuracy(labels, argmax_idx)\n",
    "    if summary:\n",
    "        summary_loss = tf.summary.scalar(\"loss\", epoch_loss_avg)\n",
    "        summary_vloss = tf.summary.scalar(\"vloss\", epoch_loss_avg)\n",
    "        summary_acc = tf.summary.scalar(\"acc\", epoch_accuracy)\n",
    "        summary_vacc = tf.summary.scalar(\"vacc\", epoch_accuracy)\n",
    "        summary_train = tf.summary.merge([summary_loss, summary_acc])\n",
    "        summary_val = tf.summary.merge([summary_vloss, summary_vacc])\n",
    "    \n",
    "with tf.variable_scope('Initializer'):\n",
    "    init_global = tf.global_variables_initializer()\n",
    "    init_local = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc04421e3e614560a42ed7ac8d76a91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='epoch', max=49)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32c7f5f8c6c48548e28d4d500a6fd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='step', max=24)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e688f2d80f1449892d77471902ebd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='val_step', max=24)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1488\n",
      "0.361\n",
      "0.4628\n",
      "0.5076\n",
      "0.5412\n",
      "0.57\n",
      "0.6104\n",
      "0.6624\n",
      "0.7022\n",
      "0.7292\n",
      "0.7464\n",
      "0.7586\n",
      "0.7666\n",
      "0.7768\n",
      "0.7848\n",
      "0.7924\n",
      "0.8\n",
      "0.8064\n",
      "0.8106\n",
      "0.8156\n",
      "0.8184\n",
      "0.8224\n",
      "0.8272\n",
      "0.8318\n",
      "0.8364\n",
      "0.8396\n",
      "0.843\n",
      "0.8446\n",
      "0.8472\n",
      "0.8486\n",
      "0.8506\n",
      "0.8518\n",
      "0.8528\n",
      "0.8548\n",
      "0.8558\n",
      "0.856\n",
      "0.8578\n",
      "0.8582\n",
      "0.859\n",
      "0.8602\n",
      "0.8608\n",
      "0.8618\n",
      "0.8628\n",
      "0.8632\n",
      "0.8638\n",
      "0.8654\n",
      "0.8656\n",
      "0.866\n",
      "0.8658\n",
      "0.8662\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "progress_epoch = IntProgress(description='epoch', min=0, max=epochs-1)\n",
    "progress_step = IntProgress(description='step', min=0, max=5000//batch_size-1)\n",
    "progress_valstep = IntProgress(description='val_step', min=0, max=5000//batch_size-1)\n",
    "display(progress_epoch)\n",
    "display(progress_step)\n",
    "display(progress_valstep)\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=2,\n",
    "    inter_op_parallelism_threads=2,\n",
    "    allow_soft_placement=True)\n",
    "\n",
    "with tf.Session(config = config) as sess:\n",
    "    sess.run(init_global)\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    val_handle = sess.run(val_iterator.string_handle())\n",
    "    for epoch in range(epochs):\n",
    "        progress_epoch.value = epoch\n",
    "        sess.run(init_local)\n",
    "        for step in range(steps_per_epoch):\n",
    "            progress_step.value = step\n",
    "            sess.run([train_op,\n",
    "                      total_loss,\n",
    "                      epoch_loss_avg_update,\n",
    "                      epoch_accuracy_update],\n",
    "                     feed_dict={iter_handle: train_handle})\n",
    "            \n",
    "        sess.run([init_local, val_init_op], feed_dict={iter_handle: val_handle})\n",
    "        for val_step in range(steps_per_epoch):\n",
    "            progress_valstep.value = val_step\n",
    "            sess.run([total_loss,\n",
    "                      epoch_loss_avg_update,\n",
    "                      epoch_accuracy_update],\n",
    "                     feed_dict={iter_handle: val_handle})\n",
    "        print(sess.run(epoch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
